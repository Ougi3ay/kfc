# -*- coding: utf-8 -*-
"""KFC Classifier

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cYs0T4Ev2EaFm39WMrxohYHw2LY7cbnS

KFCClassifier: Concrete implementation for classification
"""

import numpy as np
from sklearn.base import ClassifierMixin
from sklearn.utils.validation import check_is_fitted, check_array
from sklearn.linear_model import LogisticRegression
from sklearn.cluster import KMeans

from .base import BaseKFC
from .combiner import VotingCombiner

class KFCClassifier(BaseKFC, ClassifierMixin):
    """
    KFC (K-means, Fitting, Combining) Classifier.

    Parameters
    ----------
    local_estimator : estimator object, default=LogisticRegression()
        The base classifier to be fitted on each cluster.
        If None, LogisticRegression() is used.

    clusterers : list of clusterer objects, default=[KMeans(n_clusters=5)]
        A list of M clustering algorithms.
        If None, a single KMeans(n_clusters=5) is used.

    combiner : BaseCombiner object, default=VotingCombiner()
        The strategy for aggregating the M candidate predictions.
        If None, VotingCombiner() is used.
    """

    def __init__(self, local_estimator=None, clusterers=None, combiner=None):

        # Set default components if None
        if local_estimator is None:
            local_estimator = LogisticRegression()

        if clusterers is None:
            clusterers = [KMeans(n_clusters=5, n_init=10, random_state=0)]

        if combiner is None:
            combiner = VotingCombiner()

        super().__init__(
            local_estimator=local_estimator,
            clusterers=clusterers,
            combiner=combiner
        )

        # define variable
        self.classes_ = None

    def fit(self, X, y):
        """
        Fits the classifier, storing classes_ information.
        """
        self.classes_ = np.unique(y)
        return super().fit(X, y)

    def _get_estimator_type(self):
        return "classifier"
    
    def predict_proba(self, X):
        """
        Predict class probabilities.

        1. Gets the M probability predictions from the M candidate models.
        2. (C-Step) Uses the combiner's `combine_proba` method to aggregate them.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Data to predict.

        Returns
        -------
        final_probas : array-like of shape (n_samples, n_classes)
        """
        check_is_fitted(self, 'is_fitted_')
        X = check_array(X, accept_sparse=False)

        n_samples = X.shape[0]
        n_classes = len(self.classes_)
        all_probas = []  # List of (n_samples, n_classes) arrays

        for i in range(self.n_candidates_):
            cluster_model = self.cluster_models_[i]
            partition_models = self.fitted_local_models_[i]
            labels = cluster_model.predict(X)

            probas_for_this_partition = np.zeros((n_samples, n_classes), dtype=np.float32)

            for k, model_k in partition_models.items():
                mask = labels == k
                if np.any(mask):
                    probas_k = model_k.predict_proba(X[mask])
                    # Align probabilities to self.classes_
                    for idx_model_class, class_label in enumerate(model_k.classes_):
                        final_class_idx = np.searchsorted(self.classes_, class_label)
                        probas_for_this_partition[mask, final_class_idx] = probas_k[:, idx_model_class]

            all_probas.append(probas_for_this_partition)

        stacked_probas = np.stack(all_probas, axis=2)  # (n_samples, n_classes, M)
        return self.combiner.combine_proba(stacked_probas)

    def predict(self, X):
        """Predict class labels using combined probabilities."""
        proba = self.predict_proba(X)
        idx = np.argmax(proba, axis=1)
        return self.classes_[idx]